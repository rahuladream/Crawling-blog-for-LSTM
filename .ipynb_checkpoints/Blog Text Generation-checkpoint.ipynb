{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main moto to generate deep learning & machine learning content by LSTM content generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSTM network generate text\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and remove all the special & html tags\n",
    "CLEAN_BLOG = 'blog_store/clean_blog_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('blog_store/blog_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "replace() takes at least 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-7338894821f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclean_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYou can also read this article on Analytics Vidhya's Android APP\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mopen_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLEAN_BLOG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: replace() takes at least 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    clean_text = BeautifulSoup(df['content'][i]).get_text().strip('\\n')\n",
    "    url = df['url'][i]\n",
    "    clean_text.replace(\"\\nYou can also read this article on Analytics Vidhya's Android APP\", \"\")\n",
    "    open_file = open(CLEAN_BLOG, 'a')\n",
    "    writer = csv.writer(open_file)\n",
    "    writer.writerow([url, clean_text])\n",
    "print('completed cleaning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv('blog_store/clean_blog_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Distance metrics are a key part of several machine learning algorithms. These distance metrics are used in both supervised and unsupervised learning, generally to calculate the similarity between data points.\\nAn effective distance metric improves the performance of our machine learning model, whether that’s for classification tasks or clustering.\\n\\nLet’s say we want to create clusters using the K-Means Clustering or k-Nearest Neighbour algorithm to solve a classification or regression problem. How will you define the similarity between different observations here? How can we say that two points are similar to each other?\\nThis will happen if their features are similar, right? When we plot these points, they will be closer to each other in distance.\\n\\nHence, we can calculate the distance between points and then define the similarity between them. Here’s the million-dollar question – how do we calculate this distance and what are the different distance metrics in machine learning?\\nThat’s what we aim to answer in this article. We will walk through 4 types of distance metrics in machine learning and understand how they work in Python.\\n\\xa0\\n4 Types of Distance Metrics in Machine Learning\\n\\nEuclidean Distance\\nManhattan Distance\\nMinkowski Distance\\nHamming Distance\\n\\nLet’s start with the most commonly used distance metric – Euclidean Distance.\\n\\xa0\\n1. Euclidean Distance\\n\\nEuclidean Distance represents the shortest distance between two points.\\n\\nMost machine learning algorithms including K-Means use this distance metric to measure the similarity between observations. Let’s say we have two points as shown below:\\n\\nSo, the Euclidean Distance between these two points A and B will be:\\n\\nHere’s the formula for Euclidean Distance:\\n\\nWe use this formula when we are dealing with 2 dimensions. We can generalize this for an n-dimensional space as:\\n\\nWhere,\\n\\nn = number of dimensions\\npi, qi = data points\\n\\nLet’s code Euclidean Distance in Python. This will give you a better understanding of how this distance metric works.\\nWe will first import the required libraries. I will be using the SciPy library that contains pre-written codes for most of the distance functions used in Python:\\nView the code on Gist.\\n\\nThese are the two sample points which we will be using to calculate the different distance functions. Let’s now calculate the Euclidean Distance between these two points:\\nView the code on Gist.\\n\\nThis is how we can calculate the Euclidean Distance between two points in Python. Let’s now understand the second distance metric, Manhattan Distance.\\n\\xa0\\n2. Manhattan Distance\\n\\nManhattan Distance is the sum of absolute differences between points across all the dimensions.\\n\\nWe can represent Manhattan Distance as:\\n\\nSince the above representation is 2 dimensional, to calculate Manhattan Distance, we will take the sum of absolute distances in both the x and y directions. So, the Manhattan distance in a 2-dimensional space is given as:\\n\\nAnd the generalized formula for an n-dimensional space is given as:\\n\\nWhere,\\n\\nn = number of dimensions\\npi, qi = data points\\n\\nNow, we will calculate the Manhattan Distance between the two points:\\nView the code on Gist.\\n\\nNote that Manhattan Distance is also known as city block distance. SciPy has a function called cityblock that returns the Manhattan Distance between two points.\\nLet’s now look at the next distance metric – Minkowski Distance.\\n\\xa0\\n3. Minkowski Distance\\n\\nMinkowski Distance is the generalized form of Euclidean and Manhattan Distance.\\n\\nThe formula for Minkowski Distance is given as:\\n\\nHere, p represents the order of the norm. Let’s calculate the Minkowski Distance of the order 3:\\nView the code on Gist.\\n\\nThe p parameter of the Minkowski Distance metric of SciPy represents the order of the norm. When the order(p) is 1, it will represent Manhattan Distance and when the order in the above formula is 2, it will represent Euclidean Distance.\\nLet’s verify that in Python:\\nView the code on Gist.\\n\\nHere, you can see that when the order is 1, both Minkowski and Manhattan Distance are the same. Let’s verify the Euclidean Distance as well:\\nView the code on Gist.\\n\\nWhen the order is 2, we can see that Minkowski and Euclidean distances are the same.\\nSo far, we have covered the distance metrics that are used when we are dealing with continuous or numerical variables. But what if we have categorical variables? How can we decide the similarity between categorical variables? This is where we can make use of another distance metric called Hamming Distance.\\n\\xa0\\n4. Hamming Distance\\n\\nHamming Distance measures the similarity between two strings of the same length. The Hamming Distance between two strings of the same length is the number of positions at which the corresponding characters are different.\\n\\nLet’s understand the concept using an example. Let’s say we have two strings:\\n“euclidean” and “manhattan”\\nSince the length of these strings is equal, we can calculate the Hamming Distance. We will go character by character and match the strings. The first character of both the strings (e and m respectively) is different. Similarly, the second character of both the strings (u and a) is different. and so on.\\nLook carefully – seven characters are different whereas two characters (the last two characters) are similar:\\n\\nHence, the Hamming Distance here will be 7. Note that larger the Hamming Distance between two strings, more dissimilar will be those strings (and vice versa).\\nLet’s see how we can compute the Hamming Distance of two strings in Python. First, we’ll define two strings that we will be using:\\nView the code on Gist.\\nThese are the two strings “euclidean” and “manhattan” which we have seen in the example as well. Let’s now calculate the Hamming distance between these two strings:\\nView the code on Gist.\\n\\nAs we saw in the example above, the Hamming Distance between “euclidean” and “manhattan” is 7. We also saw that Hamming Distance only works when we have strings of the same length.\\nLet’s see what happens when we have strings of different lengths:\\nView the code on Gist.\\n\\nYou can see that the lengths of both the strings are different. Let’s see what will happen when we try to calculate the Hamming Distance between these two strings:\\nView the code on Gist.\\n\\nThis throws an error saying that the lengths of the arrays must be the same. Hence, Hamming distance only works when we have strings or arrays of the same length.\\nThese are some of the similarity measures or the distance matrices that are generally used in Machine Learning.\\nYou can also read this article on Analytics Vidhya's Android APP \""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spograde-recommendation-engine",
   "language": "python",
   "name": "spograde-recommendation-engine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
